import requests
from bs4 import BeautifulSoup
import re

# ----------------------------
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Header ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô Browser ‡∏à‡∏£‡∏¥‡∏á (‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏ô‡∏ö‡∏•‡πá‡∏≠‡∏Å)
# ----------------------------
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept-Language": "th-TH,th;q=0.9,en;q=0.8"
}

def scrape_text(url: str) -> str:
    """
    ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å URL ‡πÇ‡∏î‡∏¢‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô (Script, Style, Menu) ‡∏≠‡∏≠‡∏Å
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤" ‡∏à‡∏£‡∏¥‡∏á‡πÜ
    """
    
    # 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö URL ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô
    if not url.startswith("http"):
        url = "https://" + url

    try:
        # 2. ‡∏¢‡∏¥‡∏á Request ‡πÑ‡∏õ‡∏´‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ã‡∏ï‡πå
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.raise_for_status() # ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏°‡πà‡∏•‡πà‡∏° (200 OK)

        # 3. ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏ï‡πà‡∏≤‡∏á‡∏î‡∏≤‡∏ß (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡πá‡∏ö‡πÑ‡∏ó‡∏¢)
        res.encoding = res.apparent_encoding 

        # 4. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô Soup
        soup = BeautifulSoup(res.text, "html.parser")

        # 5. üßπ CLEANING: ‡∏•‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏≠‡∏Å (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å!)
        # - script/style: ‡πÇ‡∏Ñ‡πâ‡∏î‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡πÇ‡∏Ü‡∏©‡∏ì‡∏≤
        # - nav/footer: ‡πÄ‡∏°‡∏ô‡∏π‡πÅ‡∏•‡∏∞‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ß‡πá‡∏ö ‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥‡πÜ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß
        # - header: ‡∏´‡∏±‡∏ß‡πÄ‡∏ß‡πá‡∏ö
        unwanted_tags = ["script", "style", "header", "footer", "nav", "aside", "noscript", "iframe"]
        for tag in soup(unwanted_tags):
            tag.decompose() # ‡∏•‡∏ö tag ‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÄ‡∏•‡∏¢

        # 6. ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
        text = soup.get_text(separator=" ")

        # 7. ‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏•‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ, ‡∏•‡∏ö Newline ‡∏ß‡πà‡∏≤‡∏á‡πÜ)
        # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
        clean_text = re.sub(r'\s+', ' ', text).strip()

        return clean_text

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error fetching URL: {e}")
        return "" # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏î‡∏∂‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏Å‡∏±‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏û‡∏±‡∏á